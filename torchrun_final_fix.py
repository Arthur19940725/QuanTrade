#!/usr/bin/env python
"""
torchrunæœ€ç»ˆä¿®å¤æ–¹æ¡ˆ
å®Œå…¨è§£å†³Windowsä¸‹çš„æ‰€æœ‰é—®é¢˜
"""
import os
import sys
import argparse

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    # ç¦ç”¨libuv
    os.environ["USE_LIBUV"] = "0"
    os.environ["WORLD_SIZE"] = "1"
    os.environ["RANK"] = "0"
    os.environ["LOCAL_RANK"] = "0"
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355"
    
    # æ·»åŠ è·¯å¾„
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'finetune'))

def fix_model_paths():
    """ä¿®å¤æ¨¡å‹è·¯å¾„"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æ£€æŸ¥å¹¶ä¿®å¤åˆ†è¯å™¨è·¯å¾„
    tokenizer_path = os.path.join(base_dir, "models", "models--NeoQuasar--Kronos-Tokenizer-base")
    snapshot_dirs = []
    if os.path.exists(os.path.join(tokenizer_path, "snapshots")):
        snapshots_dir = os.path.join(tokenizer_path, "snapshots")
        snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
    
    if snapshot_dirs:
        actual_tokenizer_path = os.path.join(tokenizer_path, "snapshots", snapshot_dirs[0])
        print(f"âœ… æ‰¾åˆ°åˆ†è¯å™¨è·¯å¾„: {actual_tokenizer_path}")
    else:
        actual_tokenizer_path = tokenizer_path
        print(f"âš ï¸  ä½¿ç”¨é»˜è®¤åˆ†è¯å™¨è·¯å¾„: {actual_tokenizer_path}")
    
    # æ£€æŸ¥é¢„æµ‹å™¨è·¯å¾„
    predictor_path = os.path.join(base_dir, "models", "models--NeoQuasar--Kronos-small")
    snapshot_dirs = []
    if os.path.exists(os.path.join(predictor_path, "snapshots")):
        snapshots_dir = os.path.join(predictor_path, "snapshots")
        snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
    
    if snapshot_dirs:
        actual_predictor_path = os.path.join(predictor_path, "snapshots", snapshot_dirs[0])
        print(f"âœ… æ‰¾åˆ°é¢„æµ‹å™¨è·¯å¾„: {actual_predictor_path}")
    else:
        actual_predictor_path = predictor_path
        print(f"âš ï¸  ä½¿ç”¨é»˜è®¤é¢„æµ‹å™¨è·¯å¾„: {actual_predictor_path}")
    
    return actual_tokenizer_path, actual_predictor_path

def train_predictor_with_fixed_paths():
    """ä½¿ç”¨ä¿®å¤çš„è·¯å¾„è®­ç»ƒé¢„æµ‹å™¨"""
    try:
        from train_predictor import main as train_predictor_main
        from config import Config
        
        # è·å–ä¿®å¤çš„è·¯å¾„
        tokenizer_path, predictor_path = fix_model_paths()
        
        # åˆ›å»ºé…ç½®å¹¶ä¿®å¤è·¯å¾„
        config_instance = Config()
        config = config_instance.__dict__
        
        # ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
        config['pretrained_tokenizer_path'] = tokenizer_path
        config['pretrained_predictor_path'] = predictor_path
        config['finetuned_tokenizer_path'] = tokenizer_path  # æš‚æ—¶ä½¿ç”¨é¢„è®­ç»ƒçš„
        
        print("âœ… é…ç½®ä¿®å¤å®Œæˆï¼Œå¼€å§‹è®­ç»ƒé¢„æµ‹å™¨...")
        train_predictor_main(config)
        print("âœ… é¢„æµ‹å™¨è®­ç»ƒå®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ é¢„æµ‹å™¨è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def train_tokenizer_with_fixed_paths():
    """ä½¿ç”¨ä¿®å¤çš„è·¯å¾„è®­ç»ƒåˆ†è¯å™¨"""
    try:
        from train_tokenizer import main as train_tokenizer_main
        from config import Config
        
        # è·å–ä¿®å¤çš„è·¯å¾„
        tokenizer_path, predictor_path = fix_model_paths()
        
        # åˆ›å»ºé…ç½®å¹¶ä¿®å¤è·¯å¾„
        config_instance = Config()
        config = config_instance.__dict__
        
        # ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
        config['pretrained_tokenizer_path'] = tokenizer_path
        
        print("âœ… é…ç½®ä¿®å¤å®Œæˆï¼Œå¼€å§‹è®­ç»ƒåˆ†è¯å™¨...")
        train_tokenizer_main(config)
        print("âœ… åˆ†è¯å™¨è®­ç»ƒå®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Kronosè®­ç»ƒæœ€ç»ˆä¿®å¤æ–¹æ¡ˆ")
    parser.add_argument("--standalone", action="store_true", help="å•æœºæ¨¡å¼")
    parser.add_argument("--nproc_per_node", type=int, default=1, help="æ¯ä¸ªèŠ‚ç‚¹çš„è¿›ç¨‹æ•°")
    parser.add_argument("--train-tokenizer", action="store_true", help="è®­ç»ƒåˆ†è¯å™¨")
    parser.add_argument("--train-predictor", action="store_true", help="è®­ç»ƒé¢„æµ‹å™¨")
    parser.add_argument("script", nargs="?", help="è„šæœ¬è·¯å¾„")
    
    args = parser.parse_args()
    
    print("ğŸš€ Kronosè®­ç»ƒæœ€ç»ˆä¿®å¤æ–¹æ¡ˆ")
    print("ğŸ“‹ è§£å†³Windowsä¸‹æ‰€æœ‰torchruné—®é¢˜")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    success = False
    
    if args.train_predictor or (args.script and "train_predictor" in args.script):
        success = train_predictor_with_fixed_paths()
    elif args.train_tokenizer or (args.script and "train_tokenizer" in args.script):
        success = train_tokenizer_with_fixed_paths()
    else:
        print("âŒ è¯·æŒ‡å®šè®­ç»ƒä»»åŠ¡:")
        print("   --train-predictor: è®­ç»ƒé¢„æµ‹å™¨")
        print("   --train-tokenizer: è®­ç»ƒåˆ†è¯å™¨")
        print("   æˆ–æŒ‡å®šè„šæœ¬è·¯å¾„")
        return 1
    
    if success:
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("âŒ è®­ç»ƒå¤±è´¥!")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)