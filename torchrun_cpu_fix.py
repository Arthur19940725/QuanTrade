#!/usr/bin/env python
"""
torchrun CPUç‰ˆæœ¬ä¿®å¤æ–¹æ¡ˆ
è§£å†³Windowsä¸‹çš„libuvé—®é¢˜ï¼Œå¹¶å¼ºåˆ¶ä½¿ç”¨CPUè®­ç»ƒ
"""
import os
import sys
import argparse

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    # ç¦ç”¨libuv
    os.environ["USE_LIBUV"] = "0"
    os.environ["WORLD_SIZE"] = "1"
    os.environ["RANK"] = "0"
    os.environ["LOCAL_RANK"] = "0"
    os.environ["MASTER_ADDR"] = "localhost"
    os.environ["MASTER_PORT"] = "12355"
    
    # å¼ºåˆ¶ä½¿ç”¨CPU
    os.environ["CUDA_VISIBLE_DEVICES"] = ""
    
    # æ·»åŠ è·¯å¾„
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'finetune'))

def fix_model_paths():
    """ä¿®å¤æ¨¡å‹è·¯å¾„"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æ£€æŸ¥å¹¶ä¿®å¤åˆ†è¯å™¨è·¯å¾„
    tokenizer_path = os.path.join(base_dir, "models", "models--NeoQuasar--Kronos-Tokenizer-base")
    snapshot_dirs = []
    if os.path.exists(os.path.join(tokenizer_path, "snapshots")):
        snapshots_dir = os.path.join(tokenizer_path, "snapshots")
        snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
    
    if snapshot_dirs:
        actual_tokenizer_path = os.path.join(tokenizer_path, "snapshots", snapshot_dirs[0])
        print(f"âœ… æ‰¾åˆ°åˆ†è¯å™¨è·¯å¾„: {actual_tokenizer_path}")
    else:
        actual_tokenizer_path = tokenizer_path
        print(f"âš ï¸  ä½¿ç”¨é»˜è®¤åˆ†è¯å™¨è·¯å¾„: {actual_tokenizer_path}")
    
    # æ£€æŸ¥é¢„æµ‹å™¨è·¯å¾„
    predictor_path = os.path.join(base_dir, "models", "models--NeoQuasar--Kronos-small")
    snapshot_dirs = []
    if os.path.exists(os.path.join(predictor_path, "snapshots")):
        snapshots_dir = os.path.join(predictor_path, "snapshots")
        snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
    
    if snapshot_dirs:
        actual_predictor_path = os.path.join(predictor_path, "snapshots", snapshot_dirs[0])
        print(f"âœ… æ‰¾åˆ°é¢„æµ‹å™¨è·¯å¾„: {actual_predictor_path}")
    else:
        actual_predictor_path = predictor_path
        print(f"âš ï¸  ä½¿ç”¨é»˜è®¤é¢„æµ‹å™¨è·¯å¾„: {actual_predictor_path}")
    
    return actual_tokenizer_path, actual_predictor_path

def modify_training_script_for_cpu():
    """ä¿®æ”¹è®­ç»ƒè„šæœ¬ä»¥æ”¯æŒCPUè®­ç»ƒ"""
    # ä¿®æ”¹setup_ddpå‡½æ•°å¼ºåˆ¶ä½¿ç”¨CPU
    training_utils_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'finetune', 'utils', 'training_utils.py')
    
    try:
        with open(training_utils_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ›¿æ¢CUDAç›¸å…³ä»£ç 
        if 'torch.cuda.set_device' in content:
            content = content.replace('torch.cuda.set_device(local_rank)', '# torch.cuda.set_device(local_rank)  # Disabled for CPU')
            content = content.replace('torch.cuda.set_device(0)', '# torch.cuda.set_device(0)  # Disabled for CPU')
        
        with open(training_utils_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print("âœ… è®­ç»ƒè„šæœ¬å·²ä¿®æ”¹ä¸ºCPUæ¨¡å¼")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•ä¿®æ”¹è®­ç»ƒè„šæœ¬: {e}")

def train_predictor_cpu():
    """ä½¿ç”¨CPUè®­ç»ƒé¢„æµ‹å™¨"""
    try:
        # ä¿®æ”¹è®­ç»ƒè„šæœ¬
        modify_training_script_for_cpu()
        
        from train_predictor import main as train_predictor_main
        from config import Config
        
        # è·å–ä¿®å¤çš„è·¯å¾„
        tokenizer_path, predictor_path = fix_model_paths()
        
        # åˆ›å»ºé…ç½®å¹¶ä¿®å¤è·¯å¾„
        config_instance = Config()
        config = config_instance.__dict__
        
        # ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
        config['pretrained_tokenizer_path'] = tokenizer_path
        config['pretrained_predictor_path'] = predictor_path
        config['finetuned_tokenizer_path'] = tokenizer_path
        
        # å‡å°‘æ‰¹æ¬¡å¤§å°ä»¥é€‚åº”CPU
        config['batch_size'] = 10  # å‡å°‘æ‰¹æ¬¡å¤§å°
        config['epochs'] = 5       # å‡å°‘è®­ç»ƒè½®æ•°ç”¨äºæµ‹è¯•
        
        print("âœ… é…ç½®ä¿®å¤å®Œæˆï¼Œå¼€å§‹CPUè®­ç»ƒé¢„æµ‹å™¨...")
        print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼ï¼Œè®­ç»ƒé€Ÿåº¦ä¼šè¾ƒæ…¢")
        train_predictor_main(config)
        print("âœ… é¢„æµ‹å™¨è®­ç»ƒå®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ é¢„æµ‹å™¨è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Kronos CPUè®­ç»ƒä¿®å¤æ–¹æ¡ˆ")
    parser.add_argument("--standalone", action="store_true", help="å•æœºæ¨¡å¼")
    parser.add_argument("--nproc_per_node", type=int, default=1, help="æ¯ä¸ªèŠ‚ç‚¹çš„è¿›ç¨‹æ•°")
    parser.add_argument("--train-predictor", action="store_true", help="è®­ç»ƒé¢„æµ‹å™¨")
    parser.add_argument("--cpu", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨CPU")
    parser.add_argument("script", nargs="?", help="è„šæœ¬è·¯å¾„")
    
    args = parser.parse_args()
    
    print("ğŸš€ Kronos CPUè®­ç»ƒä¿®å¤æ–¹æ¡ˆ")
    print("ğŸ“‹ è§£å†³Windowsä¸‹torchrun libuv + CUDAå…¼å®¹æ€§é—®é¢˜")
    print("ğŸ’» å¼ºåˆ¶ä½¿ç”¨CPUè®­ç»ƒ")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    
    success = False
    
    if args.train_predictor or (args.script and "train_predictor" in args.script):
        success = train_predictor_cpu()
    else:
        print("âŒ è¯·æŒ‡å®šè®­ç»ƒä»»åŠ¡:")
        print("   --train-predictor: è®­ç»ƒé¢„æµ‹å™¨")
        print("   æˆ–æŒ‡å®šè„šæœ¬è·¯å¾„")
        return 1
    
    if success:
        print("ğŸ‰ CPUè®­ç»ƒæˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("âŒ è®­ç»ƒå¤±è´¥!")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)