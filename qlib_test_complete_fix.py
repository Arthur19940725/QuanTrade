#!/usr/bin/env python
"""
qlib_test.py å®Œæ•´ä¿®å¤ç‰ˆæœ¬
è§£å†³æ¨¡å—å¯¼å…¥å’Œæ¨¡å‹è·¯å¾„é—®é¢˜
"""
import argparse
from collections import defaultdict
import os
import pickle
import sys

# ä¿®å¤Pythonè·¯å¾„é—®é¢˜
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir) if 'finetune' in current_dir else current_dir
finetune_dir = os.path.join(project_root, 'finetune')

sys.path.insert(0, project_root)
sys.path.insert(0, finetune_dir)

from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

import qlib
from qlib.backtest import backtest, executor
from qlib.config import REG_CN
from qlib.contrib.evaluate import risk_analysis
from qlib.contrib.strategy import TopkDropoutStrategy
from qlib.utils.time import Freq

from config import Config
from model.kronos import Kronos, KronosTokenizer, auto_regressive_inference

def fix_model_paths(config):
    """ä¿®å¤æ¨¡å‹è·¯å¾„"""
    print("ğŸ”§ ä¿®å¤æ¨¡å‹è·¯å¾„...")
    
    # æ£€æŸ¥å¹¶ä¿®å¤åˆ†è¯å™¨è·¯å¾„
    tokenizer_path = config['tokenizer_path']
    if not os.path.exists(tokenizer_path):
        print(f"âš ï¸  åˆ†è¯å™¨è·¯å¾„ä¸å­˜åœ¨: {tokenizer_path}")
        
        # å°è¯•ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨
        pretrained_tokenizer = config.get('pretrained_tokenizer_path', './models/models--NeoQuasar--Kronos-Tokenizer-base')
        
        # æ£€æŸ¥snapshotè·¯å¾„
        if os.path.exists(pretrained_tokenizer):
            snapshots_dir = os.path.join(pretrained_tokenizer, 'snapshots')
            if os.path.exists(snapshots_dir):
                snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                if snapshot_dirs:
                    actual_tokenizer_path = os.path.join(snapshots_dir, snapshot_dirs[0])
                    config['tokenizer_path'] = actual_tokenizer_path
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨: {actual_tokenizer_path}")
                else:
                    config['tokenizer_path'] = pretrained_tokenizer
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨: {pretrained_tokenizer}")
            else:
                config['tokenizer_path'] = pretrained_tokenizer
                print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨: {pretrained_tokenizer}")
    
    # æ£€æŸ¥å¹¶ä¿®å¤é¢„æµ‹å™¨è·¯å¾„
    model_path = config['model_path']
    if not os.path.exists(model_path):
        print(f"âš ï¸  é¢„æµ‹å™¨è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        # å°è¯•ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨
        pretrained_predictor = config.get('pretrained_predictor_path', './models/models--NeoQuasar--Kronos-small')
        
        # æ£€æŸ¥snapshotè·¯å¾„
        if os.path.exists(pretrained_predictor):
            snapshots_dir = os.path.join(pretrained_predictor, 'snapshots')
            if os.path.exists(snapshots_dir):
                snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                if snapshot_dirs:
                    actual_predictor_path = os.path.join(snapshots_dir, snapshot_dirs[0])
                    config['model_path'] = actual_predictor_path
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨: {actual_predictor_path}")
                else:
                    config['model_path'] = pretrained_predictor
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨: {pretrained_predictor}")
            else:
                config['model_path'] = pretrained_predictor
                print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨: {pretrained_predictor}")
    
    return config

class QlibInferenceDataset(Dataset):
    """ç”¨äºQlibæ•°æ®æ¨ç†çš„æ•°æ®é›†ç±»"""
    
    def __init__(self, data_dict, lookback_window, predict_window, clip=5.0):
        self.data_dict = data_dict
        self.lookback_window = lookback_window
        self.predict_window = predict_window
        self.clip = clip
        
        # å‡†å¤‡æ ·æœ¬
        self.samples = []
        for symbol, df in data_dict.items():
            if len(df) >= lookback_window + predict_window:
                # æ ‡å‡†åŒ–æ•°æ®
                df_values = df.values
                df_normalized = np.clip(df_values, -clip, clip)
                
                # åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬
                for i in range(len(df_normalized) - lookback_window - predict_window + 1):
                    sample = {
                        'input': df_normalized[i:i+lookback_window],
                        'target': df_normalized[i+lookback_window:i+lookback_window+predict_window],
                        'symbol': symbol,
                        'index': i
                    }
                    self.samples.append(sample)
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        return {
            'input': torch.FloatTensor(sample['input']),
            'target': torch.FloatTensor(sample['target']),
            'symbol': sample['symbol'],
            'index': sample['index']
        }

def load_models(config, device):
    """åŠ è½½æ¨¡å‹"""
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹åˆ°è®¾å¤‡: {device}")
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        print(f"ğŸ“ åˆ†è¯å™¨è·¯å¾„: {config['tokenizer_path']}")
        tokenizer = KronosTokenizer.from_pretrained(config['tokenizer_path'])
        tokenizer = tokenizer.to(device).eval()
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # åŠ è½½é¢„æµ‹å™¨
        print(f"ğŸ“ é¢„æµ‹å™¨è·¯å¾„: {config['model_path']}")
        model = Kronos.from_pretrained(config['model_path'])
        model = model.to(device).eval()
        print("âœ… é¢„æµ‹å™¨åŠ è½½æˆåŠŸ")
        
        return tokenizer, model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def generate_predictions(config, test_data, device):
    """ç”Ÿæˆé¢„æµ‹"""
    print("ğŸ”„ å¼€å§‹ç”Ÿæˆé¢„æµ‹...")
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_models(config, device)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = QlibInferenceDataset(
        test_data, 
        config['lookback_window'], 
        config['predict_window'],
        config['clip']
    )
    
    print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset, 
        batch_size=min(config['batch_size'], 10),  # é™åˆ¶æ‰¹æ¬¡å¤§å°
        shuffle=False
    )
    
    predictions = {}
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="ç”Ÿæˆé¢„æµ‹")):
            try:
                inputs = batch['input'].to(device)
                targets = batch['target']
                symbols = batch['symbol']
                
                # æ‰§è¡Œæ¨ç†
                # éœ€è¦åˆ›å»ºæ—¶é—´æˆ³ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                batch_size, seq_len, feature_dim = inputs.shape
                x_stamp = torch.zeros(batch_size, seq_len, 5)  # 5ä¸ªæ—¶é—´ç‰¹å¾
                y_stamp = torch.zeros(batch_size, config['pred_len'], 5)
                
                pred_outputs = auto_regressive_inference(
                    tokenizer, model, inputs,
                    x_stamp, y_stamp,
                    config['max_context'],
                    config['pred_len'],
                    clip=config['clip'],
                    T=config['T'],
                    top_k=config['top_k'],
                    top_p=config['top_p'],
                    sample_count=1,
                    verbose=False
                )
                
                # å­˜å‚¨é¢„æµ‹ç»“æœ
                for i, symbol in enumerate(symbols):
                    if symbol not in predictions:
                        predictions[symbol] = []
                    
                    # å¤„ç†é¢„æµ‹è¾“å‡ºï¼ˆå¯èƒ½æ˜¯numpyæ•°ç»„æˆ–tensorï¼‰
                    if hasattr(pred_outputs[i], 'cpu'):
                        pred_array = pred_outputs[i].cpu().numpy()
                    else:
                        pred_array = pred_outputs[i]
                    
                    predictions[symbol].append({
                        'prediction': pred_array,
                        'target': targets[i].numpy(),
                        'index': batch['index'][i].item()
                    })
                    
            except Exception as e:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} æ¨ç†å¤±è´¥: {e}")
                continue
    
    print(f"âœ… é¢„æµ‹å®Œæˆï¼Œå¤„ç†äº† {len(predictions)} ä¸ªè‚¡ç¥¨")
    return predictions

def evaluate_predictions(predictions):
    """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    print("ğŸ“ˆ è¯„ä¼°é¢„æµ‹ç»“æœ...")
    
    all_predictions = []
    all_targets = []
    
    for symbol, preds in predictions.items():
        for pred_data in preds:
            all_predictions.append(pred_data['prediction'])
            all_targets.append(pred_data['target'])
    
    if all_predictions:
        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mse = np.mean((all_predictions - all_targets) ** 2)
        mae = np.mean(np.abs(all_predictions - all_targets))
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"   å‡æ–¹è¯¯å·® (MSE): {mse:.6f}")
        print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.6f}")
        print(f"   é¢„æµ‹æ ·æœ¬æ•°: {len(all_predictions)}")
        
        return {
            'mse': mse,
            'mae': mae,
            'num_samples': len(all_predictions)
        }
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Qlibæµ‹è¯•è„šæœ¬å®Œæ•´ä¿®å¤ç‰ˆæœ¬")
    parser.add_argument("--device", type=str, default="cpu", help="è®¾å¤‡ç±»å‹")
    
    args = parser.parse_args()
    
    print("ğŸš€ Qlibæµ‹è¯•è„šæœ¬å®Œæ•´ä¿®å¤ç‰ˆæœ¬")
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {args.device}")
    
    # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
    if args.device.startswith('cuda'):
        if not torch.cuda.is_available():
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
            device = torch.device('cpu')
        else:
            try:
                device = torch.device(args.device)
                # æµ‹è¯•è®¾å¤‡æ˜¯å¦å¯ç”¨
                torch.zeros(1).to(device)
                print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
            except Exception as e:
                print(f"âš ï¸  è®¾å¤‡ {args.device} ä¸å¯ç”¨: {e}")
                print("åˆ‡æ¢åˆ°CPU")
                device = torch.device('cpu')
    else:
        device = torch.device('cpu')
    
    # åŠ è½½é…ç½®
    try:
        config_instance = Config()
        config = config_instance.__dict__
        
        # è®¾ç½®è¿è¡Œé…ç½®
        run_config = {
            'device': str(device),
            'data_path': config['dataset_path'],
            'result_save_path': config['backtest_result_path'],
            'result_name': config['backtest_save_folder_name'],
            'tokenizer_path': config.get('finetuned_tokenizer_path', config['pretrained_tokenizer_path']),
            'model_path': config.get('finetuned_predictor_path', config['pretrained_predictor_path']),
            'max_context': config['max_context'],
            'pred_len': config['predict_window'],
            'clip': config['clip'],
            'T': config['inference_T'],
            'top_k': config['inference_top_k'],
            'top_p': config['inference_top_p'],
            'sample_count': config['inference_sample_count'],
            'batch_size': config['backtest_batch_size'],
            'lookback_window': config['lookback_window'],
            'predict_window': config['predict_window'],
            'pretrained_tokenizer_path': config['pretrained_tokenizer_path'],
            'pretrained_predictor_path': config['pretrained_predictor_path']
        }
        
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # ä¿®å¤æ¨¡å‹è·¯å¾„
    run_config = fix_model_paths(run_config)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data_path = os.path.join(run_config['data_path'], 'test_data.pkl')
    if not os.path.exists(test_data_path):
        print(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ç”Ÿæˆæµ‹è¯•æ•°æ®")
        return 1
    
    try:
        print(f"ğŸ”„ åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
        with open(test_data_path, 'rb') as f:
            test_data = pickle.load(f)
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(test_data)} ä¸ªè‚¡ç¥¨")
        
        # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®ä¿¡æ¯
        for i, (symbol, df) in enumerate(test_data.items()):
            print(f"   {symbol}: {df.shape}")
            if i >= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                break
                
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # ç”Ÿæˆé¢„æµ‹
    try:
        predictions = generate_predictions(run_config, test_data, device)
        
        if predictions:
            # è¯„ä¼°ç»“æœ
            eval_results = evaluate_predictions(predictions)
            
            if eval_results:
                print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
                return 0
            else:
                print("âŒ è¯„ä¼°å¤±è´¥")
                return 1
        else:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•é¢„æµ‹")
            return 1
            
    except Exception as e:
        print(f"âŒ é¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)