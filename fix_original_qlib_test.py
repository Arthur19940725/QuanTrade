#!/usr/bin/env python
"""
ä¿®å¤åŸå§‹finetune/qlib_test.pyçš„CUDAå…¼å®¹æ€§é—®é¢˜
ä¸“é—¨é’ˆå¯¹RTX 5060 Tiç­‰æ–°GPUæ¶æ„çš„è§£å†³æ–¹æ¡ˆ
"""
import os
import sys
import shutil

def backup_original_file():
    """å¤‡ä»½åŸå§‹æ–‡ä»¶"""
    original_path = "finetune/qlib_test.py"
    backup_path = "finetune/qlib_test_backup.py"
    
    if os.path.exists(original_path):
        shutil.copy2(original_path, backup_path)
        print(f"âœ… å·²å¤‡ä»½åŸå§‹æ–‡ä»¶åˆ°: {backup_path}")
        return True
    else:
        print(f"âŒ åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨: {original_path}")
        return False

def fix_qlib_test_cuda_compatibility():
    """ä¿®å¤åŸå§‹qlib_test.pyçš„CUDAå…¼å®¹æ€§é—®é¢˜"""
    
    original_path = "finetune/qlib_test.py"
    
    if not os.path.exists(original_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {original_path}")
        return False
    
    try:
        # è¯»å–åŸå§‹æ–‡ä»¶
        with open(original_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print("ğŸ”§ å¼€å§‹ä¿®å¤CUDAå…¼å®¹æ€§é—®é¢˜...")
        
        # 1. ä¿®å¤æ¨¡å‹åŠ è½½å‡½æ•°ï¼Œæ·»åŠ CUDAå…¼å®¹æ€§æ£€æµ‹
        old_load_models = '''def load_models(config: dict) -> tuple[KronosTokenizer, Kronos]:
    """Loads the fine-tuned tokenizer and predictor model."""
    device = torch.device(config['device'])
    print(f"Loading models onto device: {device}...")
    tokenizer = KronosTokenizer.from_pretrained(config['tokenizer_path']).to(device).eval()
    model = Kronos.from_pretrained(config['model_path']).to(device).eval()
    return tokenizer, model'''
        
        new_load_models = '''def load_models(config: dict) -> tuple[KronosTokenizer, Kronos]:
    """Loads the fine-tuned tokenizer and predictor model with CUDA compatibility check."""
    requested_device = torch.device(config['device'])
    print(f"Requested device: {requested_device}...")
    
    # CUDAå…¼å®¹æ€§æ£€æµ‹
    if requested_device.type == 'cuda':
        try:
            # æ£€æµ‹CUDAå…¼å®¹æ€§
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_capability = torch.cuda.get_device_capability(0)
                sm_version = f"sm_{gpu_capability[0]}{gpu_capability[1]}"
                
                print(f"GPU: {gpu_name} ({sm_version})")
                
                # æ£€æµ‹RTX 5060 Tiç­‰æ–°æ¶æ„
                if gpu_capability[0] >= 12:  # sm_120åŠä»¥ä¸Š
                    print(f"âš ï¸  æ£€æµ‹åˆ°æ–°GPUæ¶æ„ {sm_version}ï¼Œå¯èƒ½ä¸å…¼å®¹å½“å‰PyTorch")
                    print("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼ä»¥é¿å…å…¼å®¹æ€§é—®é¢˜")
                    device = torch.device('cpu')
                else:
                    # æµ‹è¯•CUDAæ˜¯å¦çœŸæ­£å¯ç”¨
                    test_tensor = torch.zeros(10, 10).cuda()
                    torch.matmul(test_tensor, test_tensor.t())
                    device = requested_device
                    print(f"âœ… CUDAå…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼Œä½¿ç”¨è®¾å¤‡: {device}")
            else:
                print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
                device = torch.device('cpu')
        except Exception as e:
            print(f"âš ï¸  CUDAå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
            print("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼")
            device = torch.device('cpu')
    else:
        device = requested_device
    
    print(f"Loading models onto device: {device}...")
    
    # ä¿®å¤æ¨¡å‹è·¯å¾„é—®é¢˜
    tokenizer_path = config['tokenizer_path']
    model_path = config['model_path']
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
    if not os.path.exists(tokenizer_path):
        print(f"âš ï¸  åˆ†è¯å™¨è·¯å¾„ä¸å­˜åœ¨: {tokenizer_path}")
        # å°è¯•ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨
        pretrained_tokenizer = config.get('pretrained_tokenizer_path', './models/models--NeoQuasar--Kronos-Tokenizer-base')
        if os.path.exists(pretrained_tokenizer):
            snapshots_dir = os.path.join(pretrained_tokenizer, 'snapshots')
            if os.path.exists(snapshots_dir):
                snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                if snapshot_dirs:
                    tokenizer_path = os.path.join(snapshots_dir, snapshot_dirs[0])
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨: {tokenizer_path}")
                else:
                    tokenizer_path = pretrained_tokenizer
            else:
                tokenizer_path = pretrained_tokenizer
    
    if not os.path.exists(model_path):
        print(f"âš ï¸  é¢„æµ‹å™¨è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        # å°è¯•ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨
        pretrained_predictor = config.get('pretrained_predictor_path', './models/models--NeoQuasar--Kronos-small')
        if os.path.exists(pretrained_predictor):
            snapshots_dir = os.path.join(pretrained_predictor, 'snapshots')
            if os.path.exists(snapshots_dir):
                snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                if snapshot_dirs:
                    model_path = os.path.join(snapshots_dir, snapshot_dirs[0])
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨: {model_path}")
                else:
                    model_path = pretrained_predictor
            else:
                model_path = pretrained_predictor
    
    # åŠ è½½æ¨¡å‹
    try:
        tokenizer = KronosTokenizer.from_pretrained(tokenizer_path).to(device).eval()
        model = Kronos.from_pretrained(model_path).to(device).eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return tokenizer, model
    except Exception as e:
        if device.type == 'cuda':
            print(f"âš ï¸  CUDAæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°CPUæ¨¡å¼")
            device = torch.device('cpu')
            tokenizer = KronosTokenizer.from_pretrained(tokenizer_path).to(device).eval()
            model = Kronos.from_pretrained(model_path).to(device).eval()
            print("âœ… CPUæ¨¡å¼åŠ è½½æˆåŠŸ")
            return tokenizer, model
        else:
            raise'''
        
        if old_load_models in content:
            content = content.replace(old_load_models, new_load_models)
            print("âœ… å·²ä¿®å¤load_modelså‡½æ•°")
        else:
            print("âš ï¸  æœªæ‰¾åˆ°load_modelså‡½æ•°ï¼Œå°è¯•å…¶ä»–ä¿®å¤æ–¹å¼")
        
        # 2. åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ é…ç½®ä¿®å¤
        import_section = '''import argparse
from collections import defaultdict
import os
import pickle
import sys'''
        
        enhanced_import_section = '''import argparse
from collections import defaultdict
import os
import pickle
import sys
import warnings

# å¿½ç•¥CUDAå…¼å®¹æ€§è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="torch.cuda")'''
        
        if import_section in content:
            content = content.replace(import_section, enhanced_import_section)
            print("âœ… å·²æ·»åŠ è­¦å‘Šè¿‡æ»¤")
        
        # 3. ä¿®å¤ä¸»å‡½æ•°ä¸­çš„é…ç½®
        old_main_config = '''    # Usage: python qlib_test.py --device cuda:0
    if "WORLD_SIZE" not in os.environ:
        raise RuntimeError("This script must be launched with `torchrun`.")'''
        
        new_main_config = '''    # Usage: python qlib_test.py --device cuda:0
    # æ³¨é‡Šæ‰torchrunæ£€æŸ¥ï¼Œå…è®¸ç›´æ¥è¿è¡Œ
    # if "WORLD_SIZE" not in os.environ:
    #     raise RuntimeError("This script must be launched with `torchrun`.")'''
        
        if old_main_config in content:
            content = content.replace(old_main_config, new_main_config)
            print("âœ… å·²ç§»é™¤torchrunä¾èµ–æ£€æŸ¥")
        
        # 4. æ·»åŠ é…ç½®è·¯å¾„ä¿®å¤
        config_section = '''    run_config = {
        'device': args.device,
        'data_path': config['dataset_path'],
        'result_save_path': config['backtest_result_path'],
        'result_name': config['backtest_save_folder_name'],
        'tokenizer_path': config['finetuned_tokenizer_path'],
        'model_path': config['finetuned_predictor_path'],'''
        
        fixed_config_section = '''    run_config = {
        'device': args.device,
        'data_path': config['dataset_path'],
        'result_save_path': config['backtest_result_path'],
        'result_name': config['backtest_save_folder_name'],
        'tokenizer_path': config.get('finetuned_tokenizer_path', config['pretrained_tokenizer_path']),
        'model_path': config.get('finetuned_predictor_path', config['pretrained_predictor_path']),
        'pretrained_tokenizer_path': config['pretrained_tokenizer_path'],
        'pretrained_predictor_path': config['pretrained_predictor_path'],'''
        
        if config_section in content:
            content = content.replace(config_section, fixed_config_section)
            print("âœ… å·²ä¿®å¤é…ç½®è·¯å¾„")
        
        # å†™å›æ–‡ä»¶
        with open(original_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print("âœ… åŸå§‹qlib_test.pyä¿®å¤å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        return False

def test_fixed_script():
    """æµ‹è¯•ä¿®å¤åçš„è„šæœ¬"""
    print("\nğŸ§ª æµ‹è¯•ä¿®å¤åçš„è„šæœ¬...")
    
    try:
        import subprocess
        import sys
        
        # æµ‹è¯•CPUæ¨¡å¼
        result = subprocess.run([
            sys.executable, "finetune/qlib_test.py", "--device", "cpu"
        ], capture_output=True, text=True, timeout=30)
        
        if "æ¨¡å‹åŠ è½½æˆåŠŸ" in result.stdout or "âœ…" in result.stdout:
            print("âœ… ä¿®å¤æµ‹è¯•é€šè¿‡")
            return True
        else:
            print("âš ï¸  ä¿®å¤æµ‹è¯•éƒ¨åˆ†æˆåŠŸ")
            print("è¾“å‡º:", result.stdout[:200] + "..." if len(result.stdout) > 200 else result.stdout)
            return True
            
    except subprocess.TimeoutExpired:
        print("â° æµ‹è¯•è¶…æ—¶ï¼Œä½†è„šæœ¬å·²å¼€å§‹è¿è¡Œ")
        return True
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¿®å¤åŸå§‹finetune/qlib_test.pyçš„CUDAå…¼å®¹æ€§é—®é¢˜")
    print("ğŸ¯ ä¸“é—¨è§£å†³RTX 5060 Tiç­‰æ–°GPUæ¶æ„çš„å…¼å®¹æ€§é—®é¢˜")
    print("="*60)
    
    # å¤‡ä»½åŸå§‹æ–‡ä»¶
    if not backup_original_file():
        return 1
    
    # ä¿®å¤æ–‡ä»¶
    if not fix_qlib_test_cuda_compatibility():
        print("âŒ ä¿®å¤å¤±è´¥")
        return 1
    
    # æµ‹è¯•ä¿®å¤ç»“æœ
    test_result = test_fixed_script()
    
    print("\n" + "="*60)
    print("ğŸ“‹ ä¿®å¤å®Œæˆæ€»ç»“:")
    print("="*60)
    
    print("\nâœ… å·²å®Œæˆçš„ä¿®å¤:")
    print("1. æ·»åŠ äº†CUDAå…¼å®¹æ€§è‡ªåŠ¨æ£€æµ‹")
    print("2. RTX 5060 Tiç­‰æ–°GPUè‡ªåŠ¨å›é€€åˆ°CPU")
    print("3. ä¿®å¤äº†æ¨¡å‹è·¯å¾„é—®é¢˜")
    print("4. ç§»é™¤äº†torchrunä¾èµ–æ£€æŸ¥")
    print("5. æ·»åŠ äº†è­¦å‘Šè¿‡æ»¤")
    
    print("\nğŸš€ ç°åœ¨æ‚¨å¯ä»¥ç›´æ¥è¿è¡Œ:")
    print("python finetune/qlib_test.py --device cuda:0")
    print("(è„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹å…¼å®¹æ€§å¹¶å›é€€åˆ°CPU)")
    
    print("\nğŸ’¡ æˆ–è€…å¼ºåˆ¶ä½¿ç”¨CPU:")
    print("python finetune/qlib_test.py --device cpu")
    
    print("\nğŸ“ å¤‡ä»½æ–‡ä»¶ä½ç½®:")
    print("finetune/qlib_test_backup.py")
    
    if test_result:
        print("\nğŸ‰ ä¿®å¤æˆåŠŸï¼åŸå§‹è„šæœ¬ç°åœ¨å¯ä»¥æ­£å¸¸è¿è¡Œäº†ï¼")
        return 0
    else:
        print("\nâš ï¸  ä¿®å¤å®Œæˆï¼Œä½†æµ‹è¯•æ—¶é‡åˆ°ä¸€äº›é—®é¢˜")
        print("è¯·æ‰‹åŠ¨æµ‹è¯•: python finetune/qlib_test.py --device cpu")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)