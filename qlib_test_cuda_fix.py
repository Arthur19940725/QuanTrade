#!/usr/bin/env python
"""
qlib_test.py CUDAä¿®å¤ç‰ˆæœ¬
ä¸“é—¨è§£å†³CUDAè¿è¡Œæ—¶çš„æ¨¡å‹è·¯å¾„å’Œå…¼å®¹æ€§é—®é¢˜
"""
import argparse
from collections import defaultdict
import os
import pickle
import sys

# ä¿®å¤Pythonè·¯å¾„é—®é¢˜
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir) if 'finetune' in current_dir else current_dir
finetune_dir = os.path.join(project_root, 'finetune')

sys.path.insert(0, project_root)
sys.path.insert(0, finetune_dir)

from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

import qlib
from qlib.backtest import backtest, executor
from qlib.config import REG_CN
from qlib.contrib.evaluate import risk_analysis
from qlib.contrib.strategy import TopkDropoutStrategy
from qlib.utils.time import Freq

from config import Config
from model.kronos import Kronos, KronosTokenizer, auto_regressive_inference

def check_cuda_compatibility():
    """æ£€æŸ¥CUDAå…¼å®¹æ€§"""
    print("ğŸ” æ£€æŸ¥CUDAå…¼å®¹æ€§...")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False, "cpu"
    
    print(f"âœ… CUDAå¯ç”¨ï¼Œç‰ˆæœ¬: {torch.version.cuda}")
    print(f"ğŸ–¥ï¸  GPUæ•°é‡: {torch.cuda.device_count()}")
    
    for i in range(torch.cuda.device_count()):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_capability = torch.cuda.get_device_capability(i)
        print(f"   GPU {i}: {gpu_name} (è®¡ç®—èƒ½åŠ›: sm_{gpu_capability[0]}{gpu_capability[1]})")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…¼å®¹æ€§è­¦å‘Š
    try:
        test_tensor = torch.zeros(1).cuda()
        print("âœ… CUDAå¼ é‡åˆ›å»ºæˆåŠŸ")
        return True, "cuda:0"
    except Exception as e:
        print(f"âš ï¸  CUDAå…¼å®¹æ€§é—®é¢˜: {e}")
        print("ğŸ’¡ å»ºè®®ä½¿ç”¨CPUæ¨¡å¼")
        return False, "cpu"

def fix_model_paths_for_cuda(config):
    """ä¸ºCUDAè¿è¡Œä¿®å¤æ¨¡å‹è·¯å¾„"""
    print("ğŸ”§ ä¿®å¤CUDAæ¨¡å¼ä¸‹çš„æ¨¡å‹è·¯å¾„...")
    
    # æ£€æŸ¥å¹¶ä¿®å¤åˆ†è¯å™¨è·¯å¾„
    tokenizer_path = config.get('tokenizer_path', config.get('finetuned_tokenizer_path'))
    if not tokenizer_path or not os.path.exists(tokenizer_path):
        print(f"âš ï¸  åˆ†è¯å™¨è·¯å¾„ä¸å­˜åœ¨: {tokenizer_path}")
        
        # å°è¯•ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨
        pretrained_tokenizer = config.get('pretrained_tokenizer_path', './models/models--NeoQuasar--Kronos-Tokenizer-base')
        
        # æ£€æŸ¥snapshotè·¯å¾„
        if os.path.exists(pretrained_tokenizer):
            snapshots_dir = os.path.join(pretrained_tokenizer, 'snapshots')
            if os.path.exists(snapshots_dir):
                snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                if snapshot_dirs:
                    actual_tokenizer_path = os.path.join(snapshots_dir, snapshot_dirs[0])
                    config['tokenizer_path'] = actual_tokenizer_path
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨: {actual_tokenizer_path}")
                else:
                    config['tokenizer_path'] = pretrained_tokenizer
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨: {pretrained_tokenizer}")
            else:
                config['tokenizer_path'] = pretrained_tokenizer
                print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒåˆ†è¯å™¨: {pretrained_tokenizer}")
        else:
            print(f"âŒ é¢„è®­ç»ƒåˆ†è¯å™¨è·¯å¾„ä¹Ÿä¸å­˜åœ¨: {pretrained_tokenizer}")
            return None
    
    # æ£€æŸ¥å¹¶ä¿®å¤é¢„æµ‹å™¨è·¯å¾„
    model_path = config.get('model_path', config.get('finetuned_predictor_path'))
    if not model_path or not os.path.exists(model_path):
        print(f"âš ï¸  é¢„æµ‹å™¨è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        # å°è¯•ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨
        pretrained_predictor = config.get('pretrained_predictor_path', './models/models--NeoQuasar--Kronos-small')
        
        # æ£€æŸ¥snapshotè·¯å¾„
        if os.path.exists(pretrained_predictor):
            snapshots_dir = os.path.join(pretrained_predictor, 'snapshots')
            if os.path.exists(snapshots_dir):
                snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                if snapshot_dirs:
                    actual_predictor_path = os.path.join(snapshots_dir, snapshot_dirs[0])
                    config['model_path'] = actual_predictor_path
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨: {actual_predictor_path}")
                else:
                    config['model_path'] = pretrained_predictor
                    print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨: {pretrained_predictor}")
            else:
                config['model_path'] = pretrained_predictor
                print(f"âœ… ä½¿ç”¨é¢„è®­ç»ƒé¢„æµ‹å™¨: {pretrained_predictor}")
        else:
            print(f"âŒ é¢„è®­ç»ƒé¢„æµ‹å™¨è·¯å¾„ä¹Ÿä¸å­˜åœ¨: {pretrained_predictor}")
            return None
    
    return config

def load_models_cuda_safe(config, device):
    """CUDAå®‰å…¨çš„æ¨¡å‹åŠ è½½"""
    print(f"ğŸ”„ åœ¨è®¾å¤‡ {device} ä¸Šå®‰å…¨åŠ è½½æ¨¡å‹...")
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        print(f"ğŸ“ åˆ†è¯å™¨è·¯å¾„: {config['tokenizer_path']}")
        tokenizer = KronosTokenizer.from_pretrained(config['tokenizer_path'])
        
        # å®‰å…¨åœ°ç§»åŠ¨åˆ°è®¾å¤‡
        if device.startswith('cuda'):
            try:
                tokenizer = tokenizer.cuda()
                print("âœ… åˆ†è¯å™¨æˆåŠŸç§»åŠ¨åˆ°CUDA")
            except Exception as e:
                print(f"âš ï¸  åˆ†è¯å™¨CUDAç§»åŠ¨å¤±è´¥: {e}")
                print("ğŸ”„ å›é€€åˆ°CPU")
                tokenizer = tokenizer.cpu()
                device = 'cpu'
        else:
            tokenizer = tokenizer.cpu()
        
        tokenizer = tokenizer.eval()
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # åŠ è½½é¢„æµ‹å™¨
        print(f"ğŸ“ é¢„æµ‹å™¨è·¯å¾„: {config['model_path']}")
        model = Kronos.from_pretrained(config['model_path'])
        
        # å®‰å…¨åœ°ç§»åŠ¨åˆ°è®¾å¤‡
        if device.startswith('cuda'):
            try:
                model = model.cuda()
                print("âœ… é¢„æµ‹å™¨æˆåŠŸç§»åŠ¨åˆ°CUDA")
            except Exception as e:
                print(f"âš ï¸  é¢„æµ‹å™¨CUDAç§»åŠ¨å¤±è´¥: {e}")
                print("ğŸ”„ å›é€€åˆ°CPU")
                model = model.cpu()
                device = 'cpu'
                tokenizer = tokenizer.cpu()  # ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ä¸Š
        else:
            model = model.cpu()
        
        model = model.eval()
        print("âœ… é¢„æµ‹å™¨åŠ è½½æˆåŠŸ")
        
        return tokenizer, model, device
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

class QlibInferenceDataset(Dataset):
    """CUDAä¼˜åŒ–çš„æ¨ç†æ•°æ®é›†"""
    
    def __init__(self, data_dict, lookback_window, predict_window, clip=5.0):
        self.data_dict = data_dict
        self.lookback_window = lookback_window
        self.predict_window = predict_window
        self.clip = clip
        
        # å‡†å¤‡æ ·æœ¬
        self.samples = []
        for symbol, df in data_dict.items():
            if len(df) >= lookback_window + predict_window:
                # æ ‡å‡†åŒ–æ•°æ®
                df_values = df.values.astype(np.float32)  # ä½¿ç”¨float32ä»¥èŠ‚çœGPUå†…å­˜
                df_normalized = np.clip(df_values, -clip, clip)
                
                # åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬ï¼ˆå‡å°‘æ ·æœ¬æ•°é‡ä»¥èŠ‚çœå†…å­˜ï¼‰
                step = max(1, (len(df_normalized) - lookback_window - predict_window + 1) // 10)  # æ¯10ä¸ªå–1ä¸ª
                for i in range(0, len(df_normalized) - lookback_window - predict_window + 1, step):
                    sample = {
                        'input': df_normalized[i:i+lookback_window],
                        'target': df_normalized[i+lookback_window:i+lookback_window+predict_window],
                        'symbol': symbol,
                        'index': i
                    }
                    self.samples.append(sample)
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        return {
            'input': torch.FloatTensor(sample['input']),
            'target': torch.FloatTensor(sample['target']),
            'symbol': sample['symbol'],
            'index': sample['index']
        }

def generate_predictions_cuda(config, test_data, device):
    """CUDAä¼˜åŒ–çš„é¢„æµ‹ç”Ÿæˆ"""
    print("ğŸ”„ å¼€å§‹CUDAä¼˜åŒ–é¢„æµ‹...")
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model, actual_device = load_models_cuda_safe(config, device)
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆå‡å°‘æ•°æ®é‡ä»¥é€‚åº”GPUå†…å­˜ï¼‰
    print("ğŸ“Š åˆ›å»ºä¼˜åŒ–æ•°æ®é›†...")
    dataset = QlibInferenceDataset(
        test_data, 
        config['lookback_window'], 
        config['predict_window'],
        config['clip']
    )
    
    print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå°æ‰¹æ¬¡ä»¥èŠ‚çœGPUå†…å­˜ï¼‰
    batch_size = 2 if actual_device.startswith('cuda') else 10  # CUDAä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size,
        shuffle=False,
        pin_memory=(actual_device.startswith('cuda'))
    )
    
    predictions = {}
    successful_predictions = 0
    
    print(f"ğŸš€ å¼€å§‹æ¨ç†ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}ï¼Œè®¾å¤‡: {actual_device}ï¼‰...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="CUDAæ¨ç†")):
            try:
                inputs = batch['input'].to(actual_device)
                targets = batch['target']
                symbols = batch['symbol']
                
                # åˆ›å»ºæ—¶é—´æˆ³ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                batch_size_actual, seq_len, feature_dim = inputs.shape
                x_stamp = torch.zeros(batch_size_actual, seq_len, 5).to(actual_device)
                y_stamp = torch.zeros(batch_size_actual, config['predict_window'], 5).to(actual_device)
                
                # æ‰§è¡Œæ¨ç†
                pred_outputs = auto_regressive_inference(
                    tokenizer, model, inputs,
                    x_stamp, y_stamp,
                    config['max_context'],
                    config['predict_window'],
                    clip=config['clip'],
                    T=config.get('T', 0.6),
                    top_k=config.get('top_k', 0),
                    top_p=config.get('top_p', 0.9),
                    sample_count=1,
                    verbose=False
                )
                
                # å­˜å‚¨é¢„æµ‹ç»“æœ
                for i, symbol in enumerate(symbols):
                    if symbol not in predictions:
                        predictions[symbol] = []
                    
                    # å¤„ç†é¢„æµ‹è¾“å‡º
                    if hasattr(pred_outputs[i], 'cpu'):
                        pred_array = pred_outputs[i].cpu().numpy()
                    else:
                        pred_array = pred_outputs[i]
                    
                    predictions[symbol].append({
                        'prediction': pred_array,
                        'target': targets[i].numpy(),
                        'index': batch['index'][i].item()
                    })
                    
                successful_predictions += len(symbols)
                
                # æ¯å¤„ç†10ä¸ªæ‰¹æ¬¡æ¸…ç†ä¸€æ¬¡GPUå†…å­˜
                if batch_idx % 10 == 0 and actual_device.startswith('cuda'):
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} æ¨ç†å¤±è´¥: {e}")
                # CUDAå†…å­˜ä¸è¶³æ—¶çš„å¤„ç†
                if "out of memory" in str(e).lower() and actual_device.startswith('cuda'):
                    print("ğŸ”„ GPUå†…å­˜ä¸è¶³ï¼Œæ¸…ç†ç¼“å­˜...")
                    torch.cuda.empty_cache()
                continue
    
    print(f"âœ… æ¨ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_predictions} ä¸ªæ ·æœ¬ï¼Œæ¶‰åŠ {len(predictions)} ä¸ªè‚¡ç¥¨")
    
    # æœ€ç»ˆæ¸…ç†GPUå†…å­˜
    if actual_device.startswith('cuda'):
        torch.cuda.empty_cache()
    
    return predictions

def evaluate_predictions(predictions):
    """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    print("ğŸ“ˆ è¯„ä¼°é¢„æµ‹ç»“æœ...")
    
    all_predictions = []
    all_targets = []
    
    for symbol, preds in predictions.items():
        for pred_data in preds:
            all_predictions.append(pred_data['prediction'])
            all_targets.append(pred_data['target'])
    
    if all_predictions:
        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mse = np.mean((all_predictions - all_targets) ** 2)
        mae = np.mean(np.abs(all_predictions - all_targets))
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"   å‡æ–¹è¯¯å·® (MSE): {mse:.6f}")
        print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.6f}")
        print(f"   é¢„æµ‹æ ·æœ¬æ•°: {len(all_predictions)}")
        
        return {
            'mse': mse,
            'mae': mae,
            'num_samples': len(all_predictions)
        }
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Qlibæµ‹è¯•è„šæœ¬CUDAä¿®å¤ç‰ˆæœ¬")
    parser.add_argument("--device", type=str, default="cuda:0", help="è®¾å¤‡ç±»å‹")
    parser.add_argument("--force-cpu", action="store_true", help="å¼ºåˆ¶ä½¿ç”¨CPU")
    
    args = parser.parse_args()
    
    print("ğŸš€ Qlibæµ‹è¯•è„šæœ¬CUDAä¿®å¤ç‰ˆæœ¬")
    print(f"ğŸ’» è¯·æ±‚è®¾å¤‡: {args.device}")
    
    # æ£€æŸ¥CUDAå…¼å®¹æ€§
    if args.force_cpu:
        print("âš ï¸  å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
        device = torch.device('cpu')
    else:
        cuda_ok, recommended_device = check_cuda_compatibility()
        if cuda_ok and args.device.startswith('cuda'):
            device = torch.device(args.device)
            print(f"âœ… ä½¿ç”¨CUDAè®¾å¤‡: {device}")
        else:
            device = torch.device('cpu')
            print(f"âš ï¸  å›é€€åˆ°CPUè®¾å¤‡: {device}")
    
    # åŠ è½½é…ç½®
    try:
        config_instance = Config()
        config = config_instance.__dict__
        
        # è®¾ç½®è¿è¡Œé…ç½®
        run_config = {
            'device': str(device),
            'data_path': config['dataset_path'],
            'result_save_path': config['backtest_result_path'],
            'result_name': config['backtest_save_folder_name'],
            'tokenizer_path': config.get('finetuned_tokenizer_path', config['pretrained_tokenizer_path']),
            'model_path': config.get('finetuned_predictor_path', config['pretrained_predictor_path']),
            'max_context': config['max_context'],
            'predict_window': config['predict_window'],
            'clip': config['clip'],
            'T': config.get('inference_T', 0.6),
            'top_k': config.get('inference_top_k', 0),
            'top_p': config.get('inference_top_p', 0.9),
            'sample_count': config.get('inference_sample_count', 1),
            'batch_size': config.get('backtest_batch_size', 1000),
            'lookback_window': config['lookback_window'],
            'pretrained_tokenizer_path': config['pretrained_tokenizer_path'],
            'pretrained_predictor_path': config['pretrained_predictor_path']
        }
        
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # ä¿®å¤æ¨¡å‹è·¯å¾„
    run_config = fix_model_paths_for_cuda(run_config)
    if run_config is None:
        print("âŒ æ¨¡å‹è·¯å¾„ä¿®å¤å¤±è´¥")
        return 1
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data_path = os.path.join(run_config['data_path'], 'test_data.pkl')
    if not os.path.exists(test_data_path):
        print(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ç”Ÿæˆæµ‹è¯•æ•°æ®")
        return 1
    
    try:
        print(f"ğŸ”„ åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
        with open(test_data_path, 'rb') as f:
            test_data = pickle.load(f)
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(test_data)} ä¸ªè‚¡ç¥¨")
        
        # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®ä¿¡æ¯
        for i, (symbol, df) in enumerate(test_data.items()):
            print(f"   {symbol}: {df.shape}")
            if i >= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                break
                
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # ç”Ÿæˆé¢„æµ‹
    try:
        predictions = generate_predictions_cuda(run_config, test_data, str(device))
        
        if predictions:
            # è¯„ä¼°ç»“æœ
            eval_results = evaluate_predictions(predictions)
            
            if eval_results:
                print("ğŸ‰ CUDAæµ‹è¯•å®Œæˆ!")
                return 0
            else:
                print("âŒ è¯„ä¼°å¤±è´¥")
                return 1
        else:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•é¢„æµ‹")
            return 1
            
    except Exception as e:
        print(f"âŒ é¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # å¦‚æœæ˜¯CUDAç›¸å…³é”™è¯¯ï¼Œå»ºè®®ä½¿ç”¨CPU
        if "cuda" in str(e).lower() or "gpu" in str(e).lower():
            print("\nğŸ’¡ æ£€æµ‹åˆ°CUDAç›¸å…³é”™è¯¯ï¼Œå»ºè®®å°è¯•:")
            print("   python qlib_test_cuda_fix.py --force-cpu")
        
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)