#!/usr/bin/env python
"""
qlib_test.py æ™ºèƒ½è®¾å¤‡é€‰æ‹©ç‰ˆæœ¬
è‡ªåŠ¨æ£€æµ‹CUDAå…¼å®¹æ€§ï¼Œæ™ºèƒ½é€‰æ‹©æœ€ä½³è®¾å¤‡
"""
import argparse
from collections import defaultdict
import os
import pickle
import sys
import warnings

# ä¿®å¤Pythonè·¯å¾„é—®é¢˜
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir) if 'finetune' in current_dir else current_dir
finetune_dir = os.path.join(project_root, 'finetune')

sys.path.insert(0, project_root)
sys.path.insert(0, finetune_dir)

from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

import qlib
from qlib.backtest import backtest, executor
from qlib.config import REG_CN
from qlib.contrib.evaluate import risk_analysis
from qlib.contrib.strategy import TopkDropoutStrategy
from qlib.utils.time import Freq

from config import Config
from model.kronos import Kronos, KronosTokenizer, auto_regressive_inference

def smart_device_selection(requested_device="cuda:0"):
    """æ™ºèƒ½è®¾å¤‡é€‰æ‹©ï¼Œè‡ªåŠ¨æ£€æµ‹CUDAå…¼å®¹æ€§"""
    print("ğŸ” æ™ºèƒ½è®¾å¤‡é€‰æ‹©ä¸­...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰CUDA
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return torch.device('cpu'), "CUDAä¸å¯ç”¨"
    
    print(f"âœ… CUDAå¯ç”¨ï¼Œç‰ˆæœ¬: {torch.version.cuda}")
    print(f"ğŸ–¥ï¸  GPUæ•°é‡: {torch.cuda.device_count()}")
    
    # æ£€æŸ¥GPUå…¼å®¹æ€§
    for i in range(torch.cuda.device_count()):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_capability = torch.cuda.get_device_capability(i)
        sm_version = f"sm_{gpu_capability[0]}{gpu_capability[1]}"
        print(f"   GPU {i}: {gpu_name} (è®¡ç®—èƒ½åŠ›: {sm_version})")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯RTX 5060 Tiæˆ–å…¶ä»–æ–°GPU
        if "5060" in gpu_name or gpu_capability[0] >= 12:  # sm_120åŠä»¥ä¸Š
            print(f"âš ï¸  æ£€æµ‹åˆ°æ–°GPUæ¶æ„ {sm_version}ï¼Œå¯èƒ½ä¸å½“å‰PyTorchä¸å…¼å®¹")
    
    # å°è¯•åˆ›å»ºCUDAå¼ é‡æµ‹è¯•å…¼å®¹æ€§
    print("ğŸ§ª æµ‹è¯•CUDAå…¼å®¹æ€§...")
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            test_tensor = torch.zeros(10, 10).cuda()
            result = torch.matmul(test_tensor, test_tensor.t())  # ç®€å•çš„çŸ©é˜µè¿ç®—
            print("âœ… CUDAåŸºæœ¬è¿ç®—æµ‹è¯•é€šè¿‡")
        
        # å°è¯•æ›´å¤æ‚çš„æ“ä½œ
        try:
            test_model = torch.nn.Linear(10, 5).cuda()
            output = test_model(test_tensor)
            print("âœ… CUDAç¥ç»ç½‘ç»œæµ‹è¯•é€šè¿‡")
            
            # æ¸…ç†æµ‹è¯•å¼ é‡
            del test_tensor, result, test_model, output
            torch.cuda.empty_cache()
            
            return torch.device(requested_device), "CUDAå…¼å®¹æ€§æµ‹è¯•é€šè¿‡"
            
        except Exception as e:
            print(f"âš ï¸  CUDAç¥ç»ç½‘ç»œæµ‹è¯•å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°CPUæ¨¡å¼")
            return torch.device('cpu'), f"CUDAç¥ç»ç½‘ç»œä¸å…¼å®¹: {e}"
            
    except Exception as e:
        print(f"âš ï¸  CUDAåŸºæœ¬æµ‹è¯•å¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°CPUæ¨¡å¼")
        return torch.device('cpu'), f"CUDAåŸºæœ¬è¿ç®—ä¸å…¼å®¹: {e}"

def fix_model_paths_smart(config):
    """æ™ºèƒ½ä¿®å¤æ¨¡å‹è·¯å¾„"""
    print("ğŸ”§ æ™ºèƒ½ä¿®å¤æ¨¡å‹è·¯å¾„...")
    
    # ä¿®å¤åˆ†è¯å™¨è·¯å¾„
    tokenizer_path = config.get('tokenizer_path', config.get('finetuned_tokenizer_path'))
    if not tokenizer_path or not os.path.exists(tokenizer_path):
        # æŸ¥æ‰¾é¢„è®­ç»ƒåˆ†è¯å™¨
        possible_paths = [
            config.get('pretrained_tokenizer_path'),
            './models/models--NeoQuasar--Kronos-Tokenizer-base',
            './models/models--NeoQuasar--Kronos-Tokenizer-2k'
        ]
        
        for path in possible_paths:
            if path and os.path.exists(path):
                # æ£€æŸ¥æ˜¯å¦æœ‰snapshotsç›®å½•
                snapshots_dir = os.path.join(path, 'snapshots')
                if os.path.exists(snapshots_dir):
                    snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                    if snapshot_dirs:
                        config['tokenizer_path'] = os.path.join(snapshots_dir, snapshot_dirs[0])
                        print(f"âœ… æ‰¾åˆ°åˆ†è¯å™¨: {config['tokenizer_path']}")
                        break
                else:
                    config['tokenizer_path'] = path
                    print(f"âœ… ä½¿ç”¨åˆ†è¯å™¨: {path}")
                    break
        else:
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„åˆ†è¯å™¨")
            return None
    
    # ä¿®å¤é¢„æµ‹å™¨è·¯å¾„
    model_path = config.get('model_path', config.get('finetuned_predictor_path'))
    if not model_path or not os.path.exists(model_path):
        # æŸ¥æ‰¾é¢„è®­ç»ƒé¢„æµ‹å™¨
        possible_paths = [
            config.get('pretrained_predictor_path'),
            './models/models--NeoQuasar--Kronos-small',
            './models/models--NeoQuasar--Kronos-mini',
            './models/models--NeoQuasar--Kronos-base'
        ]
        
        for path in possible_paths:
            if path and os.path.exists(path):
                # æ£€æŸ¥æ˜¯å¦æœ‰snapshotsç›®å½•
                snapshots_dir = os.path.join(path, 'snapshots')
                if os.path.exists(snapshots_dir):
                    snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                    if snapshot_dirs:
                        config['model_path'] = os.path.join(snapshots_dir, snapshot_dirs[0])
                        print(f"âœ… æ‰¾åˆ°é¢„æµ‹å™¨: {config['model_path']}")
                        break
                else:
                    config['model_path'] = path
                    print(f"âœ… ä½¿ç”¨é¢„æµ‹å™¨: {path}")
                    break
        else:
            print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„é¢„æµ‹å™¨")
            return None
    
    return config

def load_models_smart(config, device):
    """æ™ºèƒ½åŠ è½½æ¨¡å‹ï¼Œå¤„ç†è®¾å¤‡å…¼å®¹æ€§"""
    print(f"ğŸ”„ æ™ºèƒ½åŠ è½½æ¨¡å‹åˆ°è®¾å¤‡: {device}")
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        print(f"ğŸ“ åˆ†è¯å™¨è·¯å¾„: {config['tokenizer_path']}")
        tokenizer = KronosTokenizer.from_pretrained(config['tokenizer_path'])
        tokenizer = tokenizer.to(device).eval()
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # åŠ è½½é¢„æµ‹å™¨
        print(f"ğŸ“ é¢„æµ‹å™¨è·¯å¾„: {config['model_path']}")
        model = Kronos.from_pretrained(config['model_path'])
        model = model.to(device).eval()
        print("âœ… é¢„æµ‹å™¨åŠ è½½æˆåŠŸ")
        
        return tokenizer, model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        if device.type == 'cuda':
            print("ğŸ”„ å°è¯•å›é€€åˆ°CPU...")
            try:
                tokenizer = KronosTokenizer.from_pretrained(config['tokenizer_path'])
                tokenizer = tokenizer.cpu().eval()
                model = Kronos.from_pretrained(config['model_path'])
                model = model.cpu().eval()
                print("âœ… CPUæ¨¡å¼åŠ è½½æˆåŠŸ")
                return tokenizer, model
            except Exception as e2:
                print(f"âŒ CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
                raise
        else:
            raise

class SmartInferenceDataset(Dataset):
    """æ™ºèƒ½æ¨ç†æ•°æ®é›†ï¼Œæ ¹æ®è®¾å¤‡ä¼˜åŒ–"""
    
    def __init__(self, data_dict, lookback_window, predict_window, clip=5.0, device_type='cpu'):
        self.data_dict = data_dict
        self.lookback_window = lookback_window
        self.predict_window = predict_window
        self.clip = clip
        self.device_type = device_type
        
        # æ ¹æ®è®¾å¤‡ç±»å‹è°ƒæ•´é‡‡æ ·ç­–ç•¥
        sample_ratio = 0.3 if device_type == 'cpu' else 0.1  # CPUå–æ›´å¤šæ ·æœ¬ï¼ŒGPUå–å°‘é‡æ ·æœ¬
        
        self.samples = []
        for symbol, df in data_dict.items():
            if len(df) >= lookback_window + predict_window:
                df_values = df.values.astype(np.float32)
                df_normalized = np.clip(df_values, -clip, clip)
                
                # æ™ºèƒ½é‡‡æ ·
                total_possible = len(df_normalized) - lookback_window - predict_window + 1
                step = max(1, int(1 / sample_ratio))
                
                for i in range(0, total_possible, step):
                    sample = {
                        'input': df_normalized[i:i+lookback_window],
                        'target': df_normalized[i+lookback_window:i+lookback_window+predict_window],
                        'symbol': symbol,
                        'index': i
                    }
                    self.samples.append(sample)
        
        print(f"ğŸ“Š æ™ºèƒ½é‡‡æ ·å®Œæˆï¼Œæ ·æœ¬æ•°: {len(self.samples)} (è®¾å¤‡: {device_type})")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        return {
            'input': torch.FloatTensor(sample['input']),
            'target': torch.FloatTensor(sample['target']),
            'symbol': sample['symbol'],
            'index': sample['index']
        }

def generate_predictions_smart(config, test_data, device):
    """æ™ºèƒ½é¢„æµ‹ç”Ÿæˆ"""
    print("ğŸ”„ å¼€å§‹æ™ºèƒ½é¢„æµ‹ç”Ÿæˆ...")
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_models_smart(config, device)
    
    # æ ¹æ®å®é™…è®¾å¤‡ç±»å‹æ›´æ–°é…ç½®
    actual_device = next(model.parameters()).device
    print(f"ğŸ“ å®é™…ä½¿ç”¨è®¾å¤‡: {actual_device}")
    
    # åˆ›å»ºæ™ºèƒ½æ•°æ®é›†
    dataset = SmartInferenceDataset(
        test_data, 
        config['lookback_window'], 
        config['predict_window'],
        config['clip'],
        actual_device.type
    )
    
    # æ™ºèƒ½æ‰¹æ¬¡å¤§å°é€‰æ‹©
    if actual_device.type == 'cuda':
        batch_size = 1  # CUDAå…¼å®¹æ€§é—®é¢˜æ—¶ä½¿ç”¨æœ€å°æ‰¹æ¬¡
    else:
        batch_size = 8   # CPUä½¿ç”¨è¾ƒå¤§æ‰¹æ¬¡
    
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size,
        shuffle=False,
        pin_memory=(actual_device.type == 'cuda'),
        num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    predictions = {}
    successful_predictions = 0
    
    print(f"ğŸš€ å¼€å§‹æ™ºèƒ½æ¨ç†ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}ï¼Œè®¾å¤‡: {actual_device}ï¼‰...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="æ™ºèƒ½æ¨ç†")):
            try:
                inputs = batch['input'].to(actual_device)
                targets = batch['target']
                symbols = batch['symbol']
                
                # åˆ›å»ºæ—¶é—´æˆ³
                batch_size_actual, seq_len, feature_dim = inputs.shape
                x_stamp = torch.zeros(batch_size_actual, seq_len, 5).to(actual_device)
                y_stamp = torch.zeros(batch_size_actual, config['predict_window'], 5).to(actual_device)
                
                # æ‰§è¡Œæ¨ç†
                pred_outputs = auto_regressive_inference(
                    tokenizer, model, inputs,
                    x_stamp, y_stamp,
                    config['max_context'],
                    config['predict_window'],
                    clip=config['clip'],
                    T=config.get('T', 0.6),
                    top_k=config.get('top_k', 0),
                    top_p=config.get('top_p', 0.9),
                    sample_count=1,
                    verbose=False
                )
                
                # å­˜å‚¨é¢„æµ‹ç»“æœ
                for i, symbol in enumerate(symbols):
                    if symbol not in predictions:
                        predictions[symbol] = []
                    
                    if hasattr(pred_outputs[i], 'cpu'):
                        pred_array = pred_outputs[i].cpu().numpy()
                    else:
                        pred_array = pred_outputs[i]
                    
                    predictions[symbol].append({
                        'prediction': pred_array,
                        'target': targets[i].numpy(),
                        'index': batch['index'][i].item()
                    })
                    
                successful_predictions += len(symbols)
                
                # å®šæœŸæ¸…ç†GPUå†…å­˜
                if batch_idx % 5 == 0 and actual_device.type == 'cuda':
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} æ¨ç†å¤±è´¥: {e}")
                # å¦‚æœæ˜¯CUDAé”™è¯¯ï¼Œç«‹å³åœæ­¢
                if "cuda" in str(e).lower() and "kernel" in str(e).lower():
                    print("ğŸ›‘ æ£€æµ‹åˆ°CUDAå†…æ ¸é”™è¯¯ï¼Œåœæ­¢æ¨ç†")
                    break
                continue
    
    print(f"âœ… æ™ºèƒ½æ¨ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_predictions} ä¸ªæ ·æœ¬ï¼Œæ¶‰åŠ {len(predictions)} ä¸ªè‚¡ç¥¨")
    
    # æ¸…ç†å†…å­˜
    if actual_device.type == 'cuda':
        torch.cuda.empty_cache()
    
    return predictions

def evaluate_predictions(predictions):
    """è¯„ä¼°é¢„æµ‹ç»“æœ"""
    print("ğŸ“ˆ è¯„ä¼°é¢„æµ‹ç»“æœ...")
    
    all_predictions = []
    all_targets = []
    
    for symbol, preds in predictions.items():
        for pred_data in preds:
            all_predictions.append(pred_data['prediction'])
            all_targets.append(pred_data['target'])
    
    if all_predictions:
        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)
        
        mse = np.mean((all_predictions - all_targets) ** 2)
        mae = np.mean(np.abs(all_predictions - all_targets))
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"   å‡æ–¹è¯¯å·® (MSE): {mse:.6f}")
        print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.6f}")
        print(f"   é¢„æµ‹æ ·æœ¬æ•°: {len(all_predictions)}")
        
        return {
            'mse': mse,
            'mae': mae,
            'num_samples': len(all_predictions)
        }
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Qlibæµ‹è¯•è„šæœ¬æ™ºèƒ½è®¾å¤‡ç‰ˆæœ¬")
    parser.add_argument("--device", type=str, default="cuda:0", help="è¯·æ±‚çš„è®¾å¤‡ç±»å‹")
    
    args = parser.parse_args()
    
    print("ğŸš€ Qlibæµ‹è¯•è„šæœ¬æ™ºèƒ½è®¾å¤‡ç‰ˆæœ¬")
    print("ğŸ§  è‡ªåŠ¨æ£€æµ‹CUDAå…¼å®¹æ€§å¹¶æ™ºèƒ½é€‰æ‹©æœ€ä½³è®¾å¤‡")
    
    # æ™ºèƒ½è®¾å¤‡é€‰æ‹©
    device, reason = smart_device_selection(args.device)
    print(f"ğŸ’» æœ€ç»ˆé€‰æ‹©è®¾å¤‡: {device} ({reason})")
    
    # åŠ è½½é…ç½®
    try:
        config_instance = Config()
        config = config_instance.__dict__
        
        run_config = {
            'device': str(device),
            'data_path': config['dataset_path'],
            'tokenizer_path': config.get('finetuned_tokenizer_path', config['pretrained_tokenizer_path']),
            'model_path': config.get('finetuned_predictor_path', config['pretrained_predictor_path']),
            'max_context': config['max_context'],
            'predict_window': config['predict_window'],
            'clip': config['clip'],
            'T': config.get('inference_T', 0.6),
            'top_k': config.get('inference_top_k', 0),
            'top_p': config.get('inference_top_p', 0.9),
            'lookback_window': config['lookback_window'],
            'pretrained_tokenizer_path': config['pretrained_tokenizer_path'],
            'pretrained_predictor_path': config['pretrained_predictor_path']
        }
        
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # æ™ºèƒ½ä¿®å¤æ¨¡å‹è·¯å¾„
    run_config = fix_model_paths_smart(run_config)
    if run_config is None:
        print("âŒ æ¨¡å‹è·¯å¾„ä¿®å¤å¤±è´¥")
        return 1
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data_path = os.path.join(run_config['data_path'], 'test_data.pkl')
    if not os.path.exists(test_data_path):
        print(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}")
        return 1
    
    try:
        print(f"ğŸ”„ åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
        with open(test_data_path, 'rb') as f:
            test_data = pickle.load(f)
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(test_data)} ä¸ªè‚¡ç¥¨")
                
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # æ™ºèƒ½é¢„æµ‹ç”Ÿæˆ
    try:
        predictions = generate_predictions_smart(run_config, test_data, device)
        
        if predictions:
            eval_results = evaluate_predictions(predictions)
            
            if eval_results:
                print("ğŸ‰ æ™ºèƒ½æµ‹è¯•å®Œæˆ!")
                print(f"ğŸ† ä½¿ç”¨è®¾å¤‡: {device}")
                print(f"ğŸ“Š å¤„ç†æ ·æœ¬: {eval_results['num_samples']}")
                return 0
            else:
                print("âŒ è¯„ä¼°å¤±è´¥")
                return 1
        else:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•é¢„æµ‹")
            return 1
            
    except Exception as e:
        print(f"âŒ æ™ºèƒ½é¢„æµ‹å¤±è´¥: {e}")
        print("\nğŸ’¡ å»ºè®®:")
        print("1. å¦‚æœæ˜¯CUDAé—®é¢˜ï¼ŒPyTorchç‰ˆæœ¬å¯èƒ½éœ€è¦æ›´æ–°")
        print("2. æ‚¨çš„RTX 5060 Tiéœ€è¦æ›´æ–°çš„PyTorchç‰ˆæœ¬æ”¯æŒ")
        print("3. å½“å‰ç‰ˆæœ¬ä¼šè‡ªåŠ¨å›é€€åˆ°CPUï¼Œè¿™æ˜¯æ­£å¸¸çš„")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)