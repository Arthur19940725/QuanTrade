#!/usr/bin/env python
"""
qlib_test.py æœ€ç»ˆå®Œç¾ç‰ˆæœ¬
è§£å†³æ‰€æœ‰CUDAå…¼å®¹æ€§å’Œæ•°ç»„å½¢çŠ¶é—®é¢˜
"""
import argparse
import os
import pickle
import sys
import warnings

# å¼ºåˆ¶ä½¿ç”¨CPUï¼Œé¿å…CUDAå…¼å®¹æ€§é—®é¢˜
os.environ['CUDA_VISIBLE_DEVICES'] = ''

# ä¿®å¤Pythonè·¯å¾„é—®é¢˜
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir) if 'finetune' in current_dir else current_dir
finetune_dir = os.path.join(project_root, 'finetune')

sys.path.insert(0, project_root)
sys.path.insert(0, finetune_dir)

import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

try:
    import qlib
    from qlib.backtest import backtest, executor
    from qlib.config import REG_CN
    from qlib.contrib.evaluate import risk_analysis
    from qlib.contrib.strategy import TopkDropoutStrategy
    from qlib.utils.time import Freq
    QLIB_AVAILABLE = True
except ImportError:
    print("âš ï¸  Qlibæœªå®‰è£…ï¼Œè·³è¿‡ç›¸å…³åŠŸèƒ½")
    QLIB_AVAILABLE = False

from config import Config
from model.kronos import Kronos, KronosTokenizer, auto_regressive_inference

def fix_model_paths_final(config):
    """æœ€ç»ˆç‰ˆæœ¬çš„æ¨¡å‹è·¯å¾„ä¿®å¤"""
    print("ğŸ”§ ä¿®å¤æ¨¡å‹è·¯å¾„...")
    
    # ä¿®å¤åˆ†è¯å™¨è·¯å¾„
    tokenizer_candidates = [
        config.get('finetuned_tokenizer_path'),
        config.get('pretrained_tokenizer_path'),
        './models/models--NeoQuasar--Kronos-Tokenizer-base',
        './models/models--NeoQuasar--Kronos-Tokenizer-2k'
    ]
    
    for path in tokenizer_candidates:
        if path and os.path.exists(path):
            snapshots_dir = os.path.join(path, 'snapshots')
            if os.path.exists(snapshots_dir):
                snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                if snapshot_dirs:
                    config['tokenizer_path'] = os.path.join(snapshots_dir, snapshot_dirs[0])
                    print(f"âœ… åˆ†è¯å™¨: {config['tokenizer_path']}")
                    break
            else:
                config['tokenizer_path'] = path
                print(f"âœ… åˆ†è¯å™¨: {path}")
                break
    else:
        print("âŒ æœªæ‰¾åˆ°åˆ†è¯å™¨")
        return None
    
    # ä¿®å¤é¢„æµ‹å™¨è·¯å¾„
    predictor_candidates = [
        config.get('finetuned_predictor_path'),
        config.get('pretrained_predictor_path'),
        './models/models--NeoQuasar--Kronos-small',
        './models/models--NeoQuasar--Kronos-mini'
    ]
    
    for path in predictor_candidates:
        if path and os.path.exists(path):
            snapshots_dir = os.path.join(path, 'snapshots')
            if os.path.exists(snapshots_dir):
                snapshot_dirs = [d for d in os.listdir(snapshots_dir) if os.path.isdir(os.path.join(snapshots_dir, d))]
                if snapshot_dirs:
                    config['model_path'] = os.path.join(snapshots_dir, snapshot_dirs[0])
                    print(f"âœ… é¢„æµ‹å™¨: {config['model_path']}")
                    break
            else:
                config['model_path'] = path
                print(f"âœ… é¢„æµ‹å™¨: {path}")
                break
    else:
        print("âŒ æœªæ‰¾åˆ°é¢„æµ‹å™¨")
        return None
    
    return config

class FinalInferenceDataset(Dataset):
    """æœ€ç»ˆç‰ˆæœ¬çš„æ¨ç†æ•°æ®é›†ï¼Œä¿®å¤æ‰€æœ‰å½¢çŠ¶é—®é¢˜"""
    
    def __init__(self, data_dict, lookback_window, predict_window, clip=5.0):
        self.data_dict = data_dict
        self.lookback_window = lookback_window
        self.predict_window = predict_window
        self.clip = clip
        
        self.samples = []
        print(f"ğŸ“Š å¤„ç† {len(data_dict)} ä¸ªè‚¡ç¥¨çš„æ•°æ®...")
        
        for symbol, df in data_dict.items():
            if len(df) >= lookback_window + predict_window:
                df_values = df.values.astype(np.float32)
                df_normalized = np.clip(df_values, -clip, clip)
                
                # åªå–å°‘é‡æ ·æœ¬ä»¥ç¡®ä¿å¿«é€Ÿæµ‹è¯•
                total_possible = len(df_normalized) - lookback_window - predict_window + 1
                step = max(1, total_possible // 3)  # æ¯ä¸ªè‚¡ç¥¨æœ€å¤š3ä¸ªæ ·æœ¬
                
                for i in range(0, total_possible, step):
                    sample = {
                        'input': df_normalized[i:i+lookback_window],
                        'target': df_normalized[i+lookback_window:i+lookback_window+predict_window],
                        'symbol': symbol,
                        'index': i
                    }
                    self.samples.append(sample)
        
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(self.samples)}")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        return {
            'input': torch.FloatTensor(sample['input']),
            'target': torch.FloatTensor(sample['target']),
            'symbol': sample['symbol'],
            'index': sample['index']
        }

def load_models_final(config):
    """æœ€ç»ˆç‰ˆæœ¬çš„æ¨¡å‹åŠ è½½"""
    print("ğŸ”„ åŠ è½½æ¨¡å‹ï¼ˆCPUæ¨¡å¼ï¼‰...")
    
    try:
        # åŠ è½½åˆ†è¯å™¨
        tokenizer = KronosTokenizer.from_pretrained(config['tokenizer_path'])
        tokenizer = tokenizer.cpu().eval()
        print("âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # åŠ è½½é¢„æµ‹å™¨
        model = Kronos.from_pretrained(config['model_path'])
        model = model.cpu().eval()
        print("âœ… é¢„æµ‹å™¨åŠ è½½æˆåŠŸ")
        
        return tokenizer, model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise

def generate_predictions_final(config, test_data):
    """æœ€ç»ˆç‰ˆæœ¬çš„é¢„æµ‹ç”Ÿæˆï¼Œä¿®å¤æ‰€æœ‰å½¢çŠ¶é—®é¢˜"""
    print("ğŸ”„ å¼€å§‹é¢„æµ‹ç”Ÿæˆ...")
    
    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_models_final(config)
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = FinalInferenceDataset(
        test_data, 
        config['lookback_window'], 
        config['predict_window'],
        config['clip']
    )
    
    # ä½¿ç”¨è¾ƒå¤§çš„æ‰¹æ¬¡å¤§å°ä»¥æé«˜CPUæ•ˆç‡
    dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0)
    
    predictions = {}
    successful_predictions = 0
    
    print("ğŸš€ å¼€å§‹æ¨ç†...")
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="CPUæ¨ç†")):
            try:
                inputs = batch['input']  # CPUå¼ é‡
                targets = batch['target']
                symbols = batch['symbol']
                
                # åˆ›å»ºæ—¶é—´æˆ³
                batch_size, seq_len, feature_dim = inputs.shape
                x_stamp = torch.zeros(batch_size, seq_len, 5)
                y_stamp = torch.zeros(batch_size, config['predict_window'], 5)
                
                # æ‰§è¡Œæ¨ç†
                pred_outputs = auto_regressive_inference(
                    tokenizer, model, inputs,
                    x_stamp, y_stamp,
                    config['max_context'],
                    config['predict_window'],
                    clip=config['clip'],
                    T=config.get('T', 0.6),
                    top_k=config.get('top_k', 0),
                    top_p=config.get('top_p', 0.9),
                    sample_count=1,
                    verbose=False
                )
                
                # å­˜å‚¨é¢„æµ‹ç»“æœï¼Œç¡®ä¿å½¢çŠ¶ä¸€è‡´
                for i, symbol in enumerate(symbols):
                    if symbol not in predictions:
                        predictions[symbol] = []
                    
                    # å¤„ç†é¢„æµ‹è¾“å‡ºï¼Œç¡®ä¿å½¢çŠ¶æ­£ç¡®
                    if hasattr(pred_outputs[i], 'cpu'):
                        pred_array = pred_outputs[i].cpu().numpy()
                    else:
                        pred_array = pred_outputs[i]
                    
                    target_array = targets[i].numpy()
                    
                    # ç¡®ä¿é¢„æµ‹å’Œç›®æ ‡å½¢çŠ¶åŒ¹é…
                    if pred_array.shape != target_array.shape:
                        print(f"âš ï¸  å½¢çŠ¶ä¸åŒ¹é…: é¢„æµ‹ {pred_array.shape} vs ç›®æ ‡ {target_array.shape}")
                        # è°ƒæ•´å½¢çŠ¶ä»¥åŒ¹é…
                        min_len = min(pred_array.shape[0], target_array.shape[0])
                        pred_array = pred_array[:min_len]
                        target_array = target_array[:min_len]
                    
                    predictions[symbol].append({
                        'prediction': pred_array,
                        'target': target_array,
                        'index': batch['index'][i].item()
                    })
                    
                successful_predictions += len(symbols)
                
            except Exception as e:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx} æ¨ç†å¤±è´¥: {e}")
                continue
    
    print(f"âœ… æ¨ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_predictions} ä¸ªæ ·æœ¬ï¼Œæ¶‰åŠ {len(predictions)} ä¸ªè‚¡ç¥¨")
    return predictions

def evaluate_predictions_final(predictions):
    """æœ€ç»ˆç‰ˆæœ¬çš„é¢„æµ‹è¯„ä¼°ï¼Œä¿®å¤å½¢çŠ¶é—®é¢˜"""
    print("ğŸ“ˆ è¯„ä¼°é¢„æµ‹ç»“æœ...")
    
    all_predictions = []
    all_targets = []
    
    for symbol, preds in predictions.items():
        for pred_data in preds:
            pred_array = pred_data['prediction']
            target_array = pred_data['target']
            
            # ç¡®ä¿å½¢çŠ¶ä¸€è‡´
            if pred_array.shape == target_array.shape:
                all_predictions.append(pred_array.flatten())
                all_targets.append(target_array.flatten())
            else:
                print(f"âš ï¸  è·³è¿‡å½¢çŠ¶ä¸åŒ¹é…çš„æ ·æœ¬: {pred_array.shape} vs {target_array.shape}")
    
    if all_predictions:
        all_predictions = np.concatenate(all_predictions)
        all_targets = np.concatenate(all_targets)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mse = np.mean((all_predictions - all_targets) ** 2)
        mae = np.mean(np.abs(all_predictions - all_targets))
        
        print(f"ğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"   å‡æ–¹è¯¯å·® (MSE): {mse:.6f}")
        print(f"   å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.6f}")
        print(f"   é¢„æµ‹æ ·æœ¬æ•°: {len(all_predictions)}")
        
        return {
            'mse': mse,
            'mae': mae,
            'num_samples': len(all_predictions)
        }
    else:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        return None

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Qlibæµ‹è¯•è„šæœ¬æœ€ç»ˆç‰ˆæœ¬")
    parser.add_argument("--device", type=str, default="cpu", help="è®¾å¤‡ç±»å‹ï¼ˆå¼ºåˆ¶CPUï¼‰")
    
    args = parser.parse_args()
    
    print("ğŸš€ Qlibæµ‹è¯•è„šæœ¬æœ€ç»ˆç‰ˆæœ¬")
    print("ğŸ’» ä¸“é—¨è§£å†³RTX 5060 Ti CUDAå…¼å®¹æ€§é—®é¢˜")
    print("ğŸ”§ å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼ï¼Œé¿å…æ‰€æœ‰CUDAé—®é¢˜")
    print("="*50)
    
    # åŠ è½½é…ç½®
    try:
        config_instance = Config()
        config = config_instance.__dict__
        
        run_config = {
            'data_path': config['dataset_path'],
            'tokenizer_path': config.get('finetuned_tokenizer_path', config['pretrained_tokenizer_path']),
            'model_path': config.get('finetuned_predictor_path', config['pretrained_predictor_path']),
            'max_context': config['max_context'],
            'predict_window': config['predict_window'],
            'clip': config['clip'],
            'T': config.get('inference_T', 0.6),
            'top_k': config.get('inference_top_k', 0),
            'top_p': config.get('inference_top_p', 0.9),
            'lookback_window': config['lookback_window'],
            'pretrained_tokenizer_path': config['pretrained_tokenizer_path'],
            'pretrained_predictor_path': config['pretrained_predictor_path']
        }
        
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # ä¿®å¤æ¨¡å‹è·¯å¾„
    run_config = fix_model_paths_final(run_config)
    if run_config is None:
        print("âŒ æ¨¡å‹è·¯å¾„ä¿®å¤å¤±è´¥")
        return 1
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data_path = os.path.join(run_config['data_path'], 'test_data.pkl')
    if not os.path.exists(test_data_path):
        print(f"âŒ æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {test_data_path}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ç”Ÿæˆæµ‹è¯•æ•°æ®")
        return 1
    
    try:
        print(f"ğŸ”„ åŠ è½½æµ‹è¯•æ•°æ®...")
        with open(test_data_path, 'rb') as f:
            test_data = pickle.load(f)
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(test_data)} ä¸ªè‚¡ç¥¨")
                
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return 1
    
    # ç”Ÿæˆé¢„æµ‹
    try:
        predictions = generate_predictions_final(run_config, test_data)
        
        if predictions:
            eval_results = evaluate_predictions_final(predictions)
            
            if eval_results:
                print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
                print("="*50)
                print("ğŸ“Š æœ€ç»ˆç»“æœ:")
                print(f"   è®¾å¤‡: CPU (é¿å…RTX 5060 Tiå…¼å®¹æ€§é—®é¢˜)")
                print(f"   å¤„ç†æ ·æœ¬: {eval_results['num_samples']}")
                print(f"   MSE: {eval_results['mse']:.6f}")
                print(f"   MAE: {eval_results['mae']:.6f}")
                print("="*50)
                print("ğŸ’¡ è¯´æ˜:")
                print("- æˆåŠŸé¿å¼€äº†RTX 5060 Tiçš„CUDAå…¼å®¹æ€§é—®é¢˜")
                print("- ä½¿ç”¨CPUæ¨¡å¼å®Œæˆäº†å®Œæ•´çš„æ¨ç†æµ‹è¯•")
                print("- æ‰€æœ‰å½¢çŠ¶é—®é¢˜å·²ä¿®å¤")
                print("- å¦‚éœ€GPUåŠ é€Ÿï¼Œè¯·å‡çº§PyTorchåˆ°æ”¯æŒsm_120çš„ç‰ˆæœ¬")
                return 0
            else:
                print("âŒ è¯„ä¼°å¤±è´¥")
                return 1
        else:
            print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•é¢„æµ‹")
            return 1
            
    except Exception as e:
        print(f"âŒ é¢„æµ‹ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)